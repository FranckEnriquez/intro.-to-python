{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2043ae",
   "metadata": {},
   "source": [
    "## Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66856d45",
   "metadata": {},
   "source": [
    "Polars es una librería de procesamiento y análisis de datos en Python que se enfoca en el rendimiento y la escalabilidad. Está diseñada para trabajar con conjuntos de datos grandes y proporciona una estructura de datos llamada DataFrame, similar a la de pandas, pero **optimizada para operaciones eficientes en datos distribuidos y paralelos.** https://www.pola.rs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa0c3b",
   "metadata": {},
   "source": [
    "Algunas características importantes:\n",
    "\n",
    "**Procesamiento distribuido y paralelo:** Polars está diseñada para aprovechar el poder del procesamiento distribuido y paralelo, lo que permite trabajar con conjuntos de datos más grandes que no cabrían en la memoria de un solo equipo. Utiliza la biblioteca Apache Arrow para gestionar datos en memoria y permite realizar operaciones en paralelo para un procesamiento eficiente.\n",
    "\n",
    "**Operaciones de datos eficientes:** Polars ofrece un conjunto completo de operaciones de datos para manipular, transformar y analizar conjuntos de datos. Puedes realizar filtrado, selección, agregación, transformaciones, uniones, ordenamientos y más. Estas operaciones están optimizadas para un rendimiento rápido y escalable.\n",
    "\n",
    "**Integración con otras librerías:** Polars se integra bien con el ecosistema de Python y se puede combinar con otras librerías populares como NumPy, pandas y matplotlib. Esto permite intercambiar datos y aprovechar las funcionalidades adicionales de estas librerías en conjunto con Polars.\n",
    "\n",
    "**Funcionalidades avanzadas:** Además de las operaciones básicas de manipulación de datos, Polars ofrece funcionalidades más avanzadas. Esto incluye operaciones de ventana (window operations) como desplazamiento, agrupación y cálculos acumulativos, así como también la capacidad de realizar operaciones de join, merge y pivot.\n",
    "\n",
    "**Uso intuitivo y legible:** Polars se diseñó con un enfoque en la legibilidad del código y una interfaz fácil de usar. La sintaxis y la API de Polars son similares a las de pandas, lo que facilita la transición para aquellos familiarizados con esta librería. Esto también ayuda a que el código sea más claro y comprensible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb5796",
   "metadata": {},
   "source": [
    "## Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef35285a",
   "metadata": {},
   "source": [
    "PySpark es una librería de Python que proporciona una interfaz para interactuar con Apache Spark, un framework de procesamiento distribuido y escalable para el análisis de datos. \n",
    "\n",
    "PySpark permite realizar operaciones avanzadas de procesamiento y análisis de datos a gran escala de manera eficiente y tolerante a fallos. https://spark.apache.org/docs/latest/api/python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a3d20f",
   "metadata": {},
   "source": [
    "**Procesamiento distribuido y escalable:** PySpark se basa en Apache Spark, que está diseñado para el procesamiento distribuido y escalable de datos. Puede manejar grandes volúmenes de datos al distribuir el procesamiento en clústeres de computadoras. Esto permite realizar cálculos en paralelo y aprovechar el poder de procesamiento combinado de varios nodos.\n",
    "\n",
    "**Resilient Distributed Datasets (RDD):** RDD es una abstracción clave en PySpark. Representa una colección inmutable y distribuida de objetos que se pueden procesar en paralelo. RDD permite realizar operaciones como filtrado, transformación y agregación en los datos distribuidos de manera eficiente.\n",
    "\n",
    "**Transformaciones y acciones:** En PySpark, las operaciones se dividen en transformaciones y acciones. Las transformaciones son operaciones perezosas (lazy operations) que definen un plan de ejecución, pero no se realizan hasta que se invoca una acción. Las acciones son operaciones que devuelven resultados o generan una salida. Esta distinción permite optimizar las operaciones y minimizar los cálculos innecesarios.\n",
    "\n",
    "**Uso de DataFrames:** PySpark también proporciona una interfaz similar a la de pandas llamada DataFrame. Los DataFrames en PySpark son estructuras de datos tabulares, similares a las tablas en una base de datos relacional o a los DataFrames en pandas. Proporcionan una forma intuitiva y eficiente de trabajar con datos estructurados y ofrecen una amplia gama de operaciones de manipulación y análisis de datos.\n",
    "\n",
    "**Soporte para diversos formatos de datos:** PySpark admite una amplia gama de formatos de datos, como CSV, JSON, Parquet y bases de datos SQL. Puedes leer y escribir datos en estos formatos utilizando PySpark, lo que facilita la integración con diferentes fuentes y destinos de datos.\n",
    "\n",
    "**Integración con el ecosistema de Spark:** PySpark se integra bien con otras bibliotecas y componentes del ecosistema de Spark. Puedes aprovechar la potencia de módulos como Spark SQL, Spark Streaming, MLlib (machine learning) y GraphX (procesamiento de gráficos) para realizar análisis avanzados y construir aplicaciones complejas sobre la plataforma Spark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
